# Performance-Analysis-of-Gradient-Boosting-Models

This project provides an insight into the performances of three popular gradient boosting algorithms namely XGBoost, LightGBM and CatBoost on datasets having varying sizes using both CPU and GPU implementations. Comparisons among the three are made on the basis of the accuracy of the results, training time and prediction time. All computations have been done on the same machine to remove any possible bias. Parameter tuning has been done using grid search in order to make sure that the computations are made as optimally as possible.
